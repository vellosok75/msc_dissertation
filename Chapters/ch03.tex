\chapter{Online Optimization}
This chapter defines, introduces and justifies online optimization in the context of accelerators. A bief overview of optimzation algorithms and their classifications is presented. The Robust Conjugate Direction Search (RCDS) algorithm is introduced as well as the other rountines from which it was derived from.

This chapter adds no novelty to the literature in optimization. It is just an overview for merely pedagogic purposes. It is mostly based on the disscussion presented by the classic Numerical Recipes, as well as Refs.
\section{Defining Online Optimzation}
Suppose we have a machine (we do) in which there is some sort of figure of merit depending on the collective state of some set of relevent components, parts or operation modes--our parameters. There is no mechanistic/deterministic or probabilistic model for the dependence of the figure of merit on the reparameters state, but we do know the parameters affect the figure of merit. We may call these relevant parameters as knobs, since we can use them to tune the figure of merit.,

Now suppose we want to tune the knobs so the figure of merit reches a certain value, or so that it is minimized or maximized. This is an optimization problem, and we might as well call the figure of merit our objective function. Since the whole system is a black-box, to measure diferent values for the objective function, i.e., to sample it, we need to change the knobs and measure it again. The tuning procedure is thus based on trial-and-error.

If we are able to devise a computer-automated strategy to seek for the desired value or extremum of the objective function, then running this program while the machine is up and working is what we define as online optimization. The program must measure the objective function and read the current state of the knobs, calculate/decide and apply the changes on the knobs, measure the objective again and evaluate and judge the quality of the changes carried out. The process is iterated until the desired outcome is reached.

This black-box, heuristic optimzation problem describes the Dynamic Aperture otpimzation problem very well. The DA is a figure of merit related to the nonlinear dynamics--in SIRIUS' case, the sextupole magnets. There is no analytical/statistical\footnote{in principle, a surrogate model could be trained to reproduce dynamic aperture given the sextupole strengths as inputs. This is not what we have done so far} model prediciting DA changes given sextupole nudges so we cannot invert the problem and tune sextupoles to A desired DA value. The tuning procedure must be based on trial-and-error.

\section{Justifying Online Optimization}
Running online optimization in a machine will find the nearest extremum (minimium/maximum). In other words, if no stochacity element is brought into the routine to diversify the search along the parameter space, it will find local, not global extrema. How can we be sure the local minima are the best solution for the optimization problem?

It seems that we will never know, but it actually does not matter. A good-performing solution is all we care about as long as other operation parameters are not affected (more details on the next chapter). But there are reasons to believe the local minima found are actually the global ones and it has to do with how accelerators are designed and the origins of deterioration of the dynamic aperture in the machine.

Because there are correction schemes for the linear dynamics in accelerators, the Dynamic Aperture, i.e. limitations to the allowed oscillation amplitudes,  arises because of perturbations acting in a nonlinear dynamics. Other than that, the only limitation would be the physical aperture\footnote{Unperturbed nonlinear motion can display no limitations to oscillation amplitudes}.  The strength and symmetry of the whole magnets lattice is decided based on simulating several possibile machine lattice configurations and evaluating parameters such as the dynamic aperture and the beam-lifetime. The best performing and viable solution (lattice) is implemented in the real machine.

In the real machine, additional errors arising from magnets misalignment or any fields deviations can (and will) introduce additional perturbations and can deteriorate the DA. The simulating procedure actually does take into consideration the existence of errors: they are introduced in the model during evaluation of the figure of merit parameters and the best performing lattice on average is chosen.

In the machine, a particular error configuration is physically realized, and we are thus dealing with one possible lattice realization, for which the optimum configuration is not that with the largest average DA or lifetime. But we expect it to be not too far from that reference configuration chosen and applied to the machine. Online optimzaiton thus consists on adjusting the sextupole lattice to the physically realized machine lattice so that it reaches its best-performing configuration.

\section{Robust Conjugate Direction Search}
In the literature, optimization routines and algorithms are usually classified according to whether they rely on the calculation of derivatives (gradient-based) or solely on the comparison of the objective function values (gradient-free). The latter can yet be classified into direct- or indirect-search methods, depending on whether the search of the extremum relies on direct comparisons of the objective function itself or from a mathematical model of it, respectively \cite{numerical_recipes}.

Both gradient-based and gradient-free strategies rely on the comparison of the objective function at different points of the parameter space. If the objective function suffers from noise this can significantly reduce the efficiency of the optimization routine \cite{numerical_recipes, huang2019beam}. In Chap. 7 of Ref.~\cite{huang2019beam}, a review of the most popular optimization algorithms shows how most of them suffer to find minima to, at least, the precision of the noise-$\sigma$ the objective function is subjected to.

The Robust Conjugate Direction Search (RCDS) algorithm is a indirect-search, gradient-free optimization algorithm introduced in Ref.~\cite{Huang:2013}. The algorithm consists of a main loop for constructing and managing optimal search directions along the knobs space (Powell's Method) and a one-dimensional optimizer responsible for a noise-aware search for the minimum along a given direction. The algorithm is capable of optimizing the objective function (find its local maximum/minimum) to at least the precision of the objective-function noise \cite{Huang:2013, huang2019beam}, being thus adequate for online optimization problems. Specifically, for accelerator controls and optimization, the algorithm has been successfully applied to optimize beam steering and optics matching during injection \cite{Huang:2013}, reducing horizontal emittance \cite{Huang:2013, Huang:2015}and optimization of dynamic aperture \cite{Huang:2013,Huang:2015,Liuzzo:IPAC2016-THPMR015,Olsson:IPAC2018-WEPAL047,yang:ipac2022-tupopt064}.
\subsection{Line methods}
Let $f(x)\in\mathbb{R}$ be the objective function depending on the single parameter $x\in\mathbb{R}$. The task of optimizing (minimizing or maximizing) $f$ can be achieved by a direct search onver the possibile values of $x$. Since maximizing a function equals to minimizing the same function multipled by $-1$, in what follows, we shall refer to minimization only.

The search for the minimum is usually preceeded by initially \textit{bracketing} the miminum: finding a triplet of points $a<b<c$ in the domain such that $f(b)$ is smaller than both $f(a)$ and $f(c)$. If $f$ is reasonably smooth, we are certain there will be a minimium in the interval $(a, c)$. Standard bracket routines for well-behaved, noiseless objective functions can be found in the literature.

Given an intial bracketed interval, the most common line-search methods are
\begin{itemize}
    \item Golden Section Search: which updates the brackets progressively shrinking it until spans only a small interval specified by the user. Usually the machine precision is used. The miminum is then found to within this tolerance.
    \item Parabolic Interpolation: where a parabola is fitted to $f(a), f(b), f(c)$. The parabola minimum takes us to the minumum or close to a it in a single leap.
\end{itemize}
For dealing with noisy objective functions, RCDS introduces a noise-aware bracketing routine and a parabolic interpolation scan over the bracket. We assume that what we measure is $y(x)=f(x) + \xi$, where $\xi\sim\mathcal{N}(\mu=0, \sigma)$ is a random variable modeling the experimental noise. Instead of seeking for points $a<b<c$ satisfying $f(b)< f(a), f(b) < f(c)$, RCDS requires a more strict condition $f(b)< f(a)+3\sigma, f(b) < f(c)+3\sigma$, where $\sigma$ is the expected noise $\sigma = \text{Var}[\xi]$.

During the line-search, the parabola is fitted within the brackets and its minimum is taken as the objective function minimum. There is also an additional comparison of the available points within the brackets used for the fitting. If any of them is considered an outlier, it is not used during the fitting.
\subsection{Powell's conjugate direction set}
With a line-optimizer at hand, optimization of the objective function $f(\vb{x})\in\mathbb{R}$ depending on $p$ parameters $x_i$ (knobs) arranged as $\vb{x} = \mqty[x_1 & \dots & x_p]^\intercal \in \mathbb{R}^p$ is a simple matter of iterating the one-dimensional minimization along the direction of each one of the $p$ unit vectors $\vu{e}_i$. That is, given an intial configuration $\vb{x}_0$, and directions $\vu{n}$, we have the one-dimensional problem to minimize $g(\delta)=f(\vb{x}_0 + \delta \vu{n})$. The minimum is  then $f(\vb{x}_0 + \delta_* \vu{n})$, where $\delta_* = \text{arg min}_\delta g(\delta)$

As can be seen in figure, scanning along each orthogonal directioon can be time consuming specially for some functions with long narrow valleys at some angle with the coordinates basis vectors. This strategy thus is suboptimial when evaluation of the objective function is expensive.

The reason why using basis vectors can be so inneficient is because optimizing along some basis vector spoils down minimization carried out in the any other of them one. So repetetion of the procedure is required. A more effieicent strategy consists on constructing a set of special non-interfering direction vectors so that minimization carried at any given direction does not spoil the minimiation performed in any other of them. We quickly present the necessary condition these non-interfering directions must satisty.

Going back to the one-dimensional problem of minimizing along a direction $\vb{u}$, $\delta_* = \text{arg min}\ g(\delta) = f(\vb{x}_{0}+\delta\vu{u})$, we know that, at the minimum, we must have vanishing derivative: $g^{\prime}(\delta_*)=\grad f(\vb{x}_{0}+ \delta_* \vu{u})\cdot \vu{u}=0$. Therefore, the gradient is perpendicular to $\vu{u}$ at $\delta_*$.

Consider now the quadratic-form approximation for the objective function around point $\vb{x}_0$, taken as the origin.
\begin{equation}
    f(\vb{x})=f(\vb{x}_0)+\grad f(\vb{x}_0)\cdot \vb{x} + \frac{1}{2}\vb{x}\cdot\vb{H}(\vb{x}_0)\cdot\vb{x},
\end{equation}
where, as usual, $(\grad f(\vb{x}_0))_i = \pdv*{f(\vb{x}_0)}{x_i}$ is the gradient, and $(\vb{H})_{ij} = \pdv*{f(\vb{x}_0)}{x_i}{x_j}$ is the Hessian matrix. Up to such approximation, differentiation of the previous expression reaveals the gradient can be approximated by
\begin{equation}
    \grad f(\vb{x}) = \grad f(\vb{p}) + \vb{H}\cdot \vb{x}
\end{equation}
and thus is changed by
$$\delta(\grad f ) = \vb{H}\cdot \delta \vb{x}$$
upon a step $\vb{\delta \vb{x}}$. Suppose we have optimzed along direction $\vb{u}$, so $\grad f(\vb{u}) = \grad f(\vb{x}_0) + \vb{H}\cdot \vb{u}$. Now, optimizing along $\vb{v}$ will be non-interfering if the gradient stays orthogonal to $\vb{v}$, that its
\begin{equation}
    \vb{v}\cdot\vb{H}\cdot\vb{u}=0.
\end{equation}

Let $\vu{u}_i$ denote our directions set. Powell proved conjugate directions can be constructed as follows
\begin{enumerate}
    \item Set the initial directions as the basis vectors: $\vu{u}_i=\vu{e}_i, i=1,\dots, p$.
    \item Save the starting point (initial parameters state) as $\vb{x}_0$;
    \item For $i=1,\dots, p$ minimize along $\vu{u}_i$. Save the minimimum as $\vb{x}_i$.
    \item For $i=1,\dots p-1$ set $\vu{u}_i\leftarrow\vu{u}_{i+1}$
    \item Set $\vb{u}_p=\vb{x}_p - \vb{x}_0$. Normalize to obtain $\vu{u}_p$.
    \item Minimize along $\vu{u}_p$. Name the found minimum as the new $\vb{x}_0$ and repeat the procedure until reaching a certain number of evaluations or until some stopping condition is reached.
\end{enumerate}

Powell proved that repeating this procedure $k$ times for a quadratic form produces a set of directions whose last $k$ vectors are mutually, pairwise conjugate, in the sense of the Hessian matrix and that $p$ iterations of the algorithm exactly minmizes the quadratic form.

There is a problem in throwing away  for $\vu{u}_1$ for $\vb{x}_p - \vb{x}_0$ every iteration  : at some point the lines start to fold up on each other and lose linear independence. As a result the function can end up minimized only within a subspace of parameter space.

At each step, you can reinitialize directions to the basis vectors, or use any new set of orthogonal directions.The somewhat counterintuitive solution suggested by Powell is to discard not necessarily $\vu{u}_1$, but the direction along which $f$ had its largest decrease. This is justified because this direction is likely have a largest component along the new proposed conjugate direction. When doing this, the resulting set of directions is no longer conjugate, but still adequate for functions with long and narrow valleys.

\todo[inline]{Investigate the following:for a simple quadratic form, does the conjugate directions match principal axis from PCA?}
