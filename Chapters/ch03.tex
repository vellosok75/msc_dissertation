\chapter{Online Optimization}
This chapter defines online optimization in the context of accelerators. An overview of optimzation algorithms and their classifications is presented. The Robust Conjugate Direction Search algorithm is introduced and some of its applications in the accelerator community are discussed.
\section{Defining Online Optimzation}
Suppose we have a machine in which there is some sort of figure of merit depending on the collective state of relevent components, parts or operation mode. There is no mechanistic, deterministic or probabolistic model for the dependence of the figure of merit based on the relevant components state, but we do know these parameters have effect over our figure of merit. We might as well call these relevant parameters our knobs, since we can use them to tune the figure of merit. The whole system is a black-box. To access diferent values for the figure of merit, i.e., to sample it, we need to change the knobs, and measure the figure of merit.

Now suppose we want to tune the knobs so the figure of merit reches a certain value, or so that it is minimized or maximized. This is an optimization problem, and we might as well call the figure of merit our objective function. If we are able to devise a computer-automated strategy to seek for the desired value or extremum of the objective function, then running this program while the machine is up and working is what we define as online optimization.

The program must read the objective function and current state of the knobs, calculate the desired changes on the knobs, perform the changes evaluate the objective and repeat this process until reaching the desired outcome.

This is exactly where we are when it comes to the Dynamic Aperture. It is a figure of merit related to the nonlinear dynamics, in our case the sextupole magnets. There is no analytical/statistical\footnote{in principle, a surrogate model could be trained to reproduce dynamic aperture given the sextupole strengths as inputs. This is not what we have done so far} model prediciting DA changes given sextupole nudges so we cannot invert the problem and tune sextupoles to the desired DA value. Therefore, online otpimzation fits this problem well.

\section{Robust Conjugate Direction Search}
In the literature, optimization routines and algorithms are usually classified according to whether they rely on the calculation of derivatives (gradient-based) or solely on the comparison of the objective function values (gradient-free). The latter can yet be classified into direct- or indirect-search methods, depending on whether the search of the extremum relies on direct comparisons of the objective function itself or from a mathematical model of it, respectively \cite{numerical_recipes}.

Both gradient-based and gradient-free strategies rely on the comparison of the objective function at different points of the parameter space. If the objective function suffers from noise this can significantly reduce the efficiency of the optimization routine \cite{numerical_recipes, huang2019beam}. In Chap. 7 of Ref.~\cite{huang2019beam}, a review of the most popular optimization algorithms shows how most of them suffer to find minima to, at least, the precision of the noise-$\sigma$ the objective function is subjected to.

The Robust Conjugate Direction Search (RCDS) algorithm is a indirect-search, gradient-free optimization algorithm introduced in Ref.~\cite{Huang:2013}. The algorithm consists of a main loop for constructing and managing optimal search directions along the knobs space (Powell's Method) and a one-dimensional optimizer responsible for a noise-aware search for the minimum along a given direction. The algorithm is capable of optimizing the objective function (find its local maximum/minimum) to at least the precision of the objective-function noise \cite{Huang:2013, huang2019beam}, being thus adequate for online optimization problems. Specifically, for accelerator controls and optimization, the algorithm has been successfully applied to optimize beam steering and optics matching during injection \cite{Huang:2013}, reducing horizontal emittance \cite{Huang:2013, Huang:2015}and optimization of dynamic aperture \cite{Huang:2013,Huang:2015,Liuzzo:IPAC2016-THPMR015,Olsson:IPAC2018-WEPAL047,yang:ipac2022-tupopt064}.
\subsection{Line methods}
Let $f(x)\in\mathbb{R}$ be the objective function depending on the single parameter $x\in\mathbb{R}$. The task of optimizing (minimizing or maximizing) $f$ can be achieved by a direct search onver the possibile values of $x$. Since maximizing a function equals to minimizing the same function multipled by $-1$, in what follows, we shall refer to minimization only.

The search for the minimum is usually preceeded by initially \textit{bracketing} the miminum: finding a triplet of points $a<b<c$ in the domain such that $f(b)$ is smaller than both $f(a)$ and $f(c)$. If $f$ is reasonably smooth, we are certain there will be a minimium in the interval $(a, c)$. Standard bracket routines for well-behaved, noiseless objective functions can be found in the literature.

Given an intial bracketed interval, the most common line-search methods are
\begin{itemize}
    \item Golden Section Search: which updates the brackets progressively shrinking it until spans only a small interval specified by the user. Usually the machine precision is used. The miminum is then found to within this tolerance.
    \item Parabolic Interpolation: where a parabola is fitted to $f(a), f(b), f(c)$. The parabola minimum takes us to the minumum or close to a it in a single leap.
\end{itemize}
For dealing with noisy objective functions, RCDS introduces a noise-aware bracketing routine and a parabolic interpolation scan over the bracket. We assume that what we measure is $y(x)=f(x) + \xi$, where $\xi\sim\mathcal{N}(\mu=0, \sigma)$ is a random variable modeling the experimental noise. Instead of seeking for points $a<b<c$ satisfying $f(b)< f(a), f(b) < f(c)$, RCDS requires a more strict condition $f(b)< f(a)+3\sigma, f(b) < f(c)+3\sigma$, where $\sigma$ is the expected noise $\sigma = \text{Var}[\xi]$.

During the line-search, the parabola is fitted within the brackets and its minimum is taken as the objective function minimum. There is also an additional comparison of the available points within the brackets used for the fitting. If any of them is considered an outlier, it is not used during the fitting.
\subsection{Powell's conjugate direction set}
With a line-optimizer at hand, optimization of the objective function $f(\vb{x})\in\mathbb{R}$ depending on $p$ parameters $x_i$ (knobs) arranged as $\vb{x} = \mqty[x_1 & \dots & x_p]^\intercal \in \mathbb{R}^p$ is a simple matter of iterating the one-dimensional minimization along the direction of each one of the $p$ unit vectors $\vu{e}_i$. That is, given an intial configuration $\vb{x}_0$, and directions $\vu{n}$, we have the one-dimensional problem to minimize $g(\delta)=f(\vb{x}_0 + \delta \vu{n})$. The minimum is  then $f(\vb{x}_0 + \delta_* \vu{n})$, where $\delta_* = \text{arg min}_\delta g(\delta)$

As can be seen in figure, scanning along each orthogonal directioon can be time consuming specially for some functions with long narrow valleys at some angle with the coordinates basis vectors. This strategy thus is suboptimial when evaluation of the objective function is expensive.

The reason why using basis vectors can be so inneficient is because optimizing along some basis vector spoils down minimization carried out in the any other of them one. So repetetion of the procedure is required. A more effieicent strategy consists on constructing a set of special non-interfering direction vectors so that minimization carried at any given direction does not spoil the minimiation performed in any other of them. We quickly present the necessary condition these non-interfering directions must satisty.

Going back to the one-dimensional problem of minimizing along a direction $\vb{u}$, $\delta_* = \text{arg min}\ g(\delta) = f(\vb{x}_{0}+\delta\vu{u})$, we know that, at the minimum, we must have vanishing derivative: $g^{\prime}(\delta_*)=\grad f(\vb{x}_{0}+ \delta_* \vu{u})\cdot \vu{u}=0$. Therefore, the gradient is perpendicular to $\vu{u}$ at $\delta_*$.

Consider now the quadratic-form approximation for the objective function around point $\vb{x}_0$, taken as the origin.
\begin{equation}
    f(\vb{x})=f(\vb{x}_0)+\grad f(\vb{x}_0)\cdot \vb{x} + \frac{1}{2}\vb{x}\cdot\vb{H}(\vb{x}_0)\cdot\vb{x},
\end{equation}
where, as usual, $(\grad f(\vb{x}_0))_i = \pdv*{f(\vb{x}_0)}{x_i}$ is the gradient, and $(\vb{H})_{ij} = \pdv*{f(\vb{x}_0)}{x_i}{x_j}$ is the Hessian matrix. Up to such approximation, differentiation of the previous expression reaveals the gradient can be approximated by
\begin{equation}
    \grad f(\vb{x}) = \grad f(\vb{p}) + \vb{H}\cdot \vb{x}
\end{equation}
and thus is changed by
$$\delta(\grad f ) = \vb{H}\cdot \delta \vb{x}$$
upon a step $\vb{\delta \vb{x}}$. Suppose we have optimzed along direction $\vb{u}$, so $\grad f(\vb{u}) = \grad f(\vb{x}_0) + \vb{H}\cdot \vb{u}$. Now, optimizing along $\vb{v}$ will be non-interfering if the gradient stays orthogonal to $\vb{v}$, that its
\begin{equation}
    \vb{v}\cdot\vb{H}\cdot\vb{u}=0.
\end{equation}

Let $\vu{u}_i$ denote our directions set. Powell proved conjugate directions can be constructed as follows
\begin{enumerate}
    \item Set the initial directions as the basis vectors: $\vu{u}_i=\vu{e}_i, i=1,\dots, p$.
    \item Save the starting point (initial parameters state) as $\vb{x}_0$;
    \item For $i=1,\dots, p$ minimize along $\vu{u}_i$. Save the minimimum as $\vb{x}_i$.
    \item For $i=1,\dots p-1$ set $\vu{u}_i\leftarrow\vu{u}_{i+1}$
    \item Set $\vb{u}_p=\vb{x}_p - \vb{x}_0$. Normalize to obtain $\vu{u}_p$.
    \item Minimize along $\vu{u}_p$. Name the found minimum as the new $\vb{x}_0$ and repeat the procedure until reaching a certain number of evaluations or until some stopping condition is reached.
\end{enumerate}

Powell proved that repeating this procedure $k$ times for a quadratic form produces a set of directions whose last $k$ vectors are mutually, pairwise conjugate, in the sense of the Hessian matrix and that $p$ iterations of the algorithm exactly minmizes the quadratic form.

There is a problem in throwing away  for $\vu{u}_1$ for $\vb{x}_p - \vb{x}_0$ every iteration  : at some point the lines start to fold up on each other and lose linear independence. As a result the function can end up minimized only within a subspace of parameter space.

At each step, you can reinitialize directions to the basis vectors, or use any new set of orthogonal directions.The somewhat counterintuitive solution suggested by Powell is to discard not necessarily $\vu{u}_1$, but the direction along which $f$ had its largest decrease. This is justified because this direction is likely have a largest component along the new proposed conjugate direction. When doing this, the resulting set of directions is no longer conjugate, but still adequate for functions with long and narrow valleys.

\todo[inline]{Investigate the following:for a simple quadratic form, does the conjugate directions match principal axis from PCA?}
